{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "776cc3bc-f40e-4949-9694-d8c08269f8c3",
      "metadata": {
        "id": "776cc3bc-f40e-4949-9694-d8c08269f8c3"
      },
      "source": [
        "# Introduction to Reinforcement Learning: Cart Pole Demo\n",
        "\n",
        "[![Open In Colab <](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ShawnHymel/reinforcement-learning-demos/blob/main/rl-demo-cartpole.ipynb)\n",
        "\n",
        "In this tutorial, we will tackle the popular \"cart pole\" problem. We use a simple virtual environment that consists of a cart that can only move in two directions: left and right. A pole is attached to the cart and is allowed to freely rotate around the cart on some axis perpendicular to the line of cart movement. The pole starts pointing up. Your job is to keep the pole in this \"up\" position by only moving the cart. Think of it like trying to balance a pole in your hand by only moving yoru hand left and right.\n",
        "\n",
        "The action space is a discrete set:\n",
        "\n",
        " * move cart left\n",
        " * move cart right\n",
        "\n",
        " You can choose either one of these actions at each time step, and note there is no \"stay still\" option.\n",
        "\n",
        " The observation space is four continuous numerical values:\n",
        "\n",
        " 0. Cart position: [-4.8, 4.8]\n",
        " 1. Cart velocity: [-Inf, Inf]\n",
        " 2. Pole angle: [-0.418 rad, 0.418 rad]\n",
        " 3. Pole angular velocity [-Inf, Inf]\n",
        "\n",
        "If the pole passes one of the angular extents (-4.18 rad or +4.18 rad), the simulation will end, and you will need to reset the environmnet.\n",
        "\n",
        "You can read about the *gymnasium* implementation of cartpole [here](https://gymnasium.farama.org/environments/classic_control/cart_pole/).\n",
        "\n",
        "This tutorial uses the *Deep Q Network* (DQN) algorithm to learn a policy that keeps the pole vertical. You can read about the Stable Baselines3 implementation and usage of DQN [here](https://stable-baselines.readthedocs.io/en/master/modules/dqn.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de6872e3-ea80-4e12-8afc-a4a9fa504ad6",
      "metadata": {
        "id": "de6872e3-ea80-4e12-8afc-a4a9fa504ad6"
      },
      "outputs": [],
      "source": [
        "!python -m pip install gymnasium==0.28.1\n",
        "!python -m pip install stable-baselines3[extra]==2.0.0a1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd826d42-b0ae-4314-8142-513e6575caec",
      "metadata": {
        "id": "fd826d42-b0ae-4314-8142-513e6575caec"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import stable_baselines3 as sb3\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "# Check versions\n",
        "print(f\"gym version: {gym.__version__}\")\n",
        "print(f\"cv2 version: {cv2.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ea082dd-1b39-47e0-be49-d7f121fc3b7f",
      "metadata": {
        "id": "1ea082dd-1b39-47e0-be49-d7f121fc3b7f"
      },
      "outputs": [],
      "source": [
        "# Create the environment\n",
        "# https://gymnasium.farama.org/api/env/\n",
        "env = gym.make('CartPole-v1', render_mode='rgb_array')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfc559d6-9cb3-4dcf-9405-5639d7496a80",
      "metadata": {
        "id": "bfc559d6-9cb3-4dcf-9405-5639d7496a80"
      },
      "outputs": [],
      "source": [
        "# Reset the cart pole environment\n",
        "# https://gymnasium.farama.org/environments/classic_control/cart_pole/\n",
        "obs, info = env.reset()\n",
        "print(obs)\n",
        "print(info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff625aea-442f-4b6a-9735-838dce2b1816",
      "metadata": {
        "id": "ff625aea-442f-4b6a-9735-838dce2b1816"
      },
      "outputs": [],
      "source": [
        "# Render the environment (render is not the observation!)\n",
        "frame = env.render()\n",
        "print(frame.shape)\n",
        "plt.imshow(frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dccc0db-b562-4a36-9046-841ab205faf8",
      "metadata": {
        "id": "9dccc0db-b562-4a36-9046-841ab205faf8"
      },
      "outputs": [],
      "source": [
        "# View environment's action and observation spaces\n",
        "# https://gymnasium.farama.org/api/spaces/fundamental/\n",
        "print(f\"Observation space: {env.observation_space}\")\n",
        "print(f\"Action space: {env.action_space}\")\n",
        "print(f\"Random observation: {env.observation_space.sample()}\")\n",
        "print(f\"Random action: {env.action_space.sample()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db8024b1-b594-4977-a347-f44c5fcc9d1f",
      "metadata": {
        "id": "db8024b1-b594-4977-a347-f44c5fcc9d1f"
      },
      "outputs": [],
      "source": [
        "# Run a few times to see the pole fall\n",
        "obs, reward, terminated, truncated, info = env.step(0)\n",
        "print(obs)\n",
        "print(reward)\n",
        "print(terminated)\n",
        "frame = env.render()\n",
        "plt.imshow(frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "811b37d4-3fdb-419a-acfc-d54c1e3357bd",
      "metadata": {
        "id": "811b37d4-3fdb-419a-acfc-d54c1e3357bd"
      },
      "outputs": [],
      "source": [
        "# Function that tests the model in the given environment\n",
        "def test_model(env, model, video=None, msg=None):\n",
        "\n",
        "    # Reset environment\n",
        "    obs, info = env.reset()\n",
        "    frame = env.render()\n",
        "    ep_len = 0\n",
        "    ep_rew = 0\n",
        "\n",
        "    # Run episode until complete\n",
        "    while True:\n",
        "\n",
        "        # Provide observation to policy to predict the next action\n",
        "        action, _ = model.predict(obs)\n",
        "\n",
        "        # Perform action, update total reward\n",
        "        obs, reward, terminated, truncated, info = env.step(action)\n",
        "        ep_rew += reward\n",
        "\n",
        "        # Record frame to video\n",
        "        if video:\n",
        "            frame = env.render()\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "            frame = cv2.putText(\n",
        "                frame,                    # Image\n",
        "                msg,                      # Text to add\n",
        "                (10, 25),                 # Origin of text in image\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, # Font\n",
        "                1,                        # Font scale\n",
        "                (0, 0, 0,),               # Color\n",
        "                2,                        # Thickness\n",
        "                cv2.LINE_AA               # Line type\n",
        "            )\n",
        "            video.write(frame)\n",
        "\n",
        "        # Increase step counter\n",
        "        ep_len += 1\n",
        "\n",
        "        # Check to see if episode has ended\n",
        "        if terminated or truncated:\n",
        "            return ep_len, ep_rew"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d129b1ac-8356-4030-a0af-db3f671b7c32",
      "metadata": {
        "id": "d129b1ac-8356-4030-a0af-db3f671b7c32"
      },
      "outputs": [],
      "source": [
        "# Model that just predicts random actions\n",
        "class DummyModel():\n",
        "\n",
        "    # Save environment\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "\n",
        "    # Always output random action regardless of observation\n",
        "    def predict(self, obs):\n",
        "        action = self.env.action_space.sample()\n",
        "        return action, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4245c84e-6d1c-4366-86a5-473e39f139d3",
      "metadata": {
        "id": "4245c84e-6d1c-4366-86a5-473e39f139d3"
      },
      "outputs": [],
      "source": [
        "# Recorder settings\n",
        "FPS = 30\n",
        "FOURCC = cv2.VideoWriter.fourcc('m', 'p', '4', 'v')\n",
        "VIDEO_FILENAME = \"1-random.mp4\"\n",
        "\n",
        "# Use frame from environment to compute resolution\n",
        "width = frame.shape[1]\n",
        "height = frame.shape[0]\n",
        "\n",
        "# Create recorder\n",
        "video = cv2.VideoWriter(VIDEO_FILENAME, FOURCC, FPS, (width, height))\n",
        "\n",
        "# Try running a few episodes with the environment and random actions\n",
        "dummy_model = DummyModel(env)\n",
        "for ep in range(5):\n",
        "    ep_len, ep_rew = test_model(env, dummy_model, video, f\"Random, episode {ep}\")\n",
        "    print(f\"Episode {ep} | length: {ep_len}, reward: {ep_rew}\")\n",
        "\n",
        "# Close the video writer\n",
        "video.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5671c8f-4dfe-4333-a91e-fa87052a827e",
      "metadata": {
        "id": "e5671c8f-4dfe-4333-a91e-fa87052a827e"
      },
      "outputs": [],
      "source": [
        "# Initialize model\n",
        "# DQN docs: https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html\n",
        "# Policy networks: https://stable-baselines.readthedocs.io/en/master/modules/policies.html\n",
        "# Hyperparameters from: https://github.com/DLR-RM/stable-baselines3/issues/526#issuecomment-891153610\n",
        "model = sb3.DQN(\n",
        "    'MlpPolicy',\n",
        "    env,\n",
        "    learning_rate=0.0001,       # Learning rate of neural network (default: 0.0001)\n",
        "    buffer_size=1000000,        # Size of the replay buffer (default: 1000000)\n",
        "    learning_starts=1000,       # Number of steps to take before learning (default: 50000)\n",
        "    batch_size=32,              # Minibatch size for each gradient update (default: 32)\n",
        "    gamma=0.99,                 # Discount factor (default: 0.99)\n",
        "    train_freq=256,             # Number of steps to take before update (default: 4)\n",
        "    gradient_steps=128,         # Number of gradient steps after each rollout (default: 1)\n",
        "    target_update_interval=10,  # Number of steps to take before target network update (default: 10000)\n",
        "    policy_kwargs={'net_arch': [256, 256]}, # 2 hidden layers, 1 output layer (default: [64, 64])\n",
        "    verbose=0                   # Print training metrics (default: 0)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac6a474e-d252-4487-9498-5024a92d2c22",
      "metadata": {
        "id": "ac6a474e-d252-4487-9498-5024a92d2c22"
      },
      "outputs": [],
      "source": [
        "# Training and testing hyperparameters\n",
        "NUM_ROUNDS = 20\n",
        "NUM_TRAINING_STEPS_PER_ROUND = 5000\n",
        "NUM_TESTS_PER_ROUND = 100\n",
        "MODEL_FILENAME_BASE = \"cartpole-dqn\"\n",
        "VIDEO_FILENAME = \"2-training.mp4\"\n",
        "\n",
        "# Create recorder\n",
        "video = cv2.VideoWriter(VIDEO_FILENAME, FOURCC, FPS, (width, height))\n",
        "\n",
        "# Train and test the model for a number of rounds\n",
        "avg_ep_lens = []\n",
        "avg_ep_rews = []\n",
        "for rnd in range(NUM_ROUNDS):\n",
        "\n",
        "    # Train the model\n",
        "    model.learn(total_timesteps=NUM_TRAINING_STEPS_PER_ROUND)\n",
        "\n",
        "    # Save the model\n",
        "    model.save(f\"{MODEL_FILENAME_BASE}_{rnd}\")\n",
        "\n",
        "    # Test the model in several episodes\n",
        "    avg_ep_len = 0\n",
        "    avg_ep_rew = 0\n",
        "    for ep in range(NUM_TESTS_PER_ROUND):\n",
        "\n",
        "        # Only record the first test\n",
        "        if ep == 0:\n",
        "            ep_len, ep_rew = test_model(env, model, video, f\"Round {rnd}\")\n",
        "        else:\n",
        "            ep_len, ep_rew = test_model(env, model)\n",
        "\n",
        "        # Accumulate average length and reward\n",
        "        avg_ep_len += ep_len\n",
        "        avg_ep_rew += ep_rew\n",
        "\n",
        "    # Record and dieplay average episode length and reward\n",
        "    avg_ep_len /= NUM_TESTS_PER_ROUND\n",
        "    avg_ep_lens.append(avg_ep_len)\n",
        "    avg_ep_rew /= NUM_TESTS_PER_ROUND\n",
        "    avg_ep_rews.append(avg_ep_rew)\n",
        "    print(f\"Round {rnd} | average test length: {avg_ep_len}, average test reward: {avg_ep_rew}\")\n",
        "\n",
        "# Close the video writer\n",
        "video.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93a8f342-a885-4b03-a03b-486477577489",
      "metadata": {
        "id": "93a8f342-a885-4b03-a03b-486477577489"
      },
      "outputs": [],
      "source": [
        "# Plot average test episode lengths and rewards for each round\n",
        "fig, axs = plt.subplots(1, 2)\n",
        "fig.tight_layout(pad=2.0)\n",
        "axs[0].plot(avg_ep_lens)\n",
        "axs[0].set_ylabel(\"Average episode length\")\n",
        "axs[0].set_xlabel(\"Round\")\n",
        "axs[1].plot(avg_ep_rews)\n",
        "axs[1].set_ylabel(\"Average episode reward\")\n",
        "axs[1].set_xlabel(\"Round\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c9f56a9-4830-4327-8aae-3881e9676162",
      "metadata": {
        "id": "4c9f56a9-4830-4327-8aae-3881e9676162"
      },
      "outputs": [],
      "source": [
        "# Model and video settings\n",
        "MODEL_FILENAME = \"cartpole-dqn_19\"\n",
        "VIDEO_FILENAME = \"3-testing.mp4\"\n",
        "\n",
        "# Load the model\n",
        "model = sb3.DQN.load(MODEL_FILENAME)\n",
        "\n",
        "# Create recorder\n",
        "video = cv2.VideoWriter(VIDEO_FILENAME, FOURCC, FPS, (width, height))\n",
        "\n",
        "# Test the model\n",
        "ep_len, ep_rew = test_model(env, model, video, MODEL_FILENAME)\n",
        "print(f\"Episode length: {ep_len}, reward: {ep_rew}\")\n",
        "\n",
        "# Close the video writer\n",
        "video.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f80a9e8b-501d-4306-8d6d-f8c1b988f4ec",
      "metadata": {
        "id": "f80a9e8b-501d-4306-8d6d-f8c1b988f4ec"
      },
      "outputs": [],
      "source": [
        "# We're done with the environment\n",
        "env.close()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}