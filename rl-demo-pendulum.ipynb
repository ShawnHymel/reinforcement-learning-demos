{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "776cc3bc-f40e-4949-9694-d8c08269f8c3",
      "metadata": {
        "id": "776cc3bc-f40e-4949-9694-d8c08269f8c3"
      },
      "source": [
        "# Introduction to Reinforcement Learning: Pendulum Demo\n",
        "\n",
        "[![Open In Colab <](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ShawnHymel/reinforcement-learning-demos/blob/main/rl-demo-pendulum.ipynb)\n",
        "\n",
        "This tutorial acts as a solution to the inverted pendulum swingup challenge at the end of the Introduction to Reinforcement Learning video. Execute each of the cells and follow the directions. That being said, I highly recommend trying to find the solution yourself prior to reading this solution!\n",
        "\n",
        "The goal is to apply some amount of torque to a freely swinging pendulum to cause it to swing up and stay in the upright position.\n",
        "\n",
        "The action space is continuous (theta is the pendulum's angle between [-pi, pi]:\n",
        "\n",
        " * Apply an amount of torque in the counter-clockwise direction to the end of the pendulum in Newton-meters between the values [-2.0, 2.0]\n",
        "\n",
        "The observation space is 3 continuous numerical values:\n",
        "\n",
        " 0. x cartesian coordinate of the pendulum's end (*=cos(theta)*): [-1.0, 1.0]\n",
        " 1. y cartesian coordinate of the pendulum's end (*=sin(theta)*): [-1.0, 1.0]\n",
        " 2. Angular velocity: [-8.0, 8.0]\n",
        "\n",
        "The reward is calculated based on the angle of the pendulum and the applied torque:\n",
        "\n",
        "*r = -(theta<sup>2</sup> + 0.1 * theta_dt<sup>2</sup> + 0.001 * torque<sup>2</sup>)*\n",
        "\n",
        "From this, we find that the minimum reward is -16.2736044 (pendulum in the downward position moving quickly and with maximum torque applied) wheres the maximum reward is 0 (pendulum in the upright position with 0 velocity and no torque applied).\n",
        "\n",
        "Read more about the pendulum environment [here](https://gymnasium.farama.org/environments/classic_control/pendulum/).\n",
        "\n",
        "This tutorial uses the *Proximal Policy Optimization* (PPO) algorithm to learn a policy that keeps the pendulum in the upright position. You can read about the Stable Baselines3 implementation and usage of PPO [here](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de6872e3-ea80-4e12-8afc-a4a9fa504ad6",
      "metadata": {
        "id": "de6872e3-ea80-4e12-8afc-a4a9fa504ad6"
      },
      "outputs": [],
      "source": [
        "!python -m pip install gymnasium==0.28.1\n",
        "!python -m pip install stable-baselines3[extra]==2.0.0a1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd826d42-b0ae-4314-8142-513e6575caec",
      "metadata": {
        "id": "fd826d42-b0ae-4314-8142-513e6575caec"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import stable_baselines3 as sb3\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Check versions\n",
        "print(f\"gym version: {gym.__version__}\")\n",
        "print(f\"cv2 version: {cv2.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ea082dd-1b39-47e0-be49-d7f121fc3b7f",
      "metadata": {
        "id": "1ea082dd-1b39-47e0-be49-d7f121fc3b7f"
      },
      "outputs": [],
      "source": [
        "# Create the environment\n",
        "# https://gymnasium.farama.org/api/env/\n",
        "env = gym.make('Pendulum-v1', render_mode='rgb_array')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfc559d6-9cb3-4dcf-9405-5639d7496a80",
      "metadata": {
        "id": "bfc559d6-9cb3-4dcf-9405-5639d7496a80"
      },
      "outputs": [],
      "source": [
        "# Reset the pendulum environment\n",
        "# https://gymnasium.farama.org/environments/classic_control/pendulum/\n",
        "obs, info = env.reset()\n",
        "print(obs)\n",
        "print(info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff625aea-442f-4b6a-9735-838dce2b1816",
      "metadata": {
        "id": "ff625aea-442f-4b6a-9735-838dce2b1816"
      },
      "outputs": [],
      "source": [
        "# Render the environment (render is not the observation!)\n",
        "frame = env.render()\n",
        "print(frame.shape)\n",
        "plt.imshow(frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dccc0db-b562-4a36-9046-841ab205faf8",
      "metadata": {
        "id": "9dccc0db-b562-4a36-9046-841ab205faf8"
      },
      "outputs": [],
      "source": [
        "# View environment's action and observation spaces\n",
        "# https://gymnasium.farama.org/api/spaces/fundamental/\n",
        "print(f\"Observation space: {env.observation_space}\")\n",
        "print(f\"Action space: {env.action_space}\")\n",
        "print(f\"Random observation: {env.observation_space.sample()}\")\n",
        "print(f\"Random action: {env.action_space.sample()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db8024b1-b594-4977-a347-f44c5fcc9d1f",
      "metadata": {
        "id": "db8024b1-b594-4977-a347-f44c5fcc9d1f"
      },
      "outputs": [],
      "source": [
        "# Run a few times to see the pole fall\n",
        "torque = np.array([0.0])\n",
        "obs, reward, terminated, truncated, info = env.step(torque)\n",
        "print(obs)\n",
        "print(reward)\n",
        "print(terminated)\n",
        "frame = env.render()\n",
        "plt.imshow(frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "811b37d4-3fdb-419a-acfc-d54c1e3357bd",
      "metadata": {
        "id": "811b37d4-3fdb-419a-acfc-d54c1e3357bd"
      },
      "outputs": [],
      "source": [
        "# Function that tests the model in the given environment\n",
        "def test_model(env, model, video=None, msg=None):\n",
        "\n",
        "    # Reset environment\n",
        "    obs, info = env.reset()\n",
        "    frame = env.render()\n",
        "    ep_len = 0\n",
        "    ep_rew = 0\n",
        "\n",
        "    # Run episode until complete\n",
        "    while True:\n",
        "\n",
        "        # Provide observation to policy to predict the next action\n",
        "        action, _ = model.predict(obs)\n",
        "\n",
        "        # Perform action, update total reward\n",
        "        obs, reward, terminated, truncated, info = env.step(action)\n",
        "        ep_rew += reward\n",
        "\n",
        "        # Record frame to video\n",
        "        if video:\n",
        "            frame = env.render()\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "            frame = cv2.putText(\n",
        "                frame,                    # Image\n",
        "                msg,                      # Text to add\n",
        "                (10, 25),                 # Origin of text in imagg\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, # Font\n",
        "                1,                        # Font scale\n",
        "                (0, 0, 0,),               # Color\n",
        "                2,                        # Thickness\n",
        "                cv2.LINE_AA               # Line type\n",
        "            )\n",
        "            video.write(frame)\n",
        "\n",
        "        # Increase step counter\n",
        "        ep_len += 1\n",
        "\n",
        "        # Check to see if episode has ended\n",
        "        if terminated or truncated:\n",
        "            return ep_len, ep_rew"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d129b1ac-8356-4030-a0af-db3f671b7c32",
      "metadata": {
        "id": "d129b1ac-8356-4030-a0af-db3f671b7c32"
      },
      "outputs": [],
      "source": [
        "# Model that just predicts random actions\n",
        "class DummyModel():\n",
        "\n",
        "    # Save environment\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "\n",
        "    # Always output random action regardless of observation\n",
        "    def predict(self, obs):\n",
        "        action = self.env.action_space.sample()\n",
        "        return action, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4245c84e-6d1c-4366-86a5-473e39f139d3",
      "metadata": {
        "id": "4245c84e-6d1c-4366-86a5-473e39f139d3"
      },
      "outputs": [],
      "source": [
        "# Recorder settings\n",
        "FPS = 30\n",
        "FOURCC = cv2.VideoWriter.fourcc('m', 'p', '4', 'v')\n",
        "VIDEO_FILENAME = \"1-random.mp4\"\n",
        "\n",
        "# Use frame from environment to compute resolution\n",
        "width = frame.shape[1]\n",
        "height = frame.shape[0]\n",
        "\n",
        "# Create recorder\n",
        "video = cv2.VideoWriter(VIDEO_FILENAME, FOURCC, FPS, (width, height))\n",
        "\n",
        "# Try running a few episodes with the environment and random actions\n",
        "dummy_model = DummyModel(env)\n",
        "for ep in range(5):\n",
        "    ep_len, ep_rew = test_model(env, dummy_model, video, f\"Random, episode {ep}\")\n",
        "    print(f\"Episode {ep} | length: {ep_len}, reward: {ep_rew}\")\n",
        "\n",
        "# Close the video writer\n",
        "video.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5671c8f-4dfe-4333-a91e-fa87052a827e",
      "metadata": {
        "id": "e5671c8f-4dfe-4333-a91e-fa87052a827e"
      },
      "outputs": [],
      "source": [
        "# Initialize model\n",
        "# PPO docs: https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html\n",
        "# Policy networks: https://stable-baselines.readthedocs.io/en/master/modules/policies.html\n",
        "# Hyperparameters from: https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/ppo.yml\n",
        "model = sb3.PPO(\n",
        "    'MlpPolicy',\n",
        "    env,\n",
        "    learning_rate=0.001,       # Learning rate of neural network (default: 0.0003)\n",
        "    n_steps=1024,               # Number of steps per update (default: 2048)\n",
        "    batch_size=64,              # Minibatch size for NN update (default: 64)\n",
        "    gamma=0.9,                 # Discount factor (default: 0.99)\n",
        "    ent_coef=0.0,               # Entropy, how much to explore (default: 0.0)\n",
        "    use_sde=True,               # Use generalized State Dependent Exploration (default: False)\n",
        "    sde_sample_freq=4,          # Number of steps before sampling new noise matrix (default -1)\n",
        "    policy_kwargs={'net_arch': [64, 64]}, # 2 hidden layers, 1 output layer (default: [64, 64])\n",
        "    verbose=0                   # Print training metrics (default: 0)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac6a474e-d252-4487-9498-5024a92d2c22",
      "metadata": {
        "id": "ac6a474e-d252-4487-9498-5024a92d2c22"
      },
      "outputs": [],
      "source": [
        "# Training and testing hyperparameters\n",
        "NUM_ROUNDS = 20\n",
        "NUM_TRAINING_STEPS_PER_ROUND = 5000\n",
        "NUM_TESTS_PER_ROUND = 100\n",
        "MODEL_FILENAME_BASE = \"pendulum-ppo\"\n",
        "VIDEO_FILENAME = \"2-training.mp4\"\n",
        "\n",
        "# Create recorder\n",
        "video = cv2.VideoWriter(VIDEO_FILENAME, FOURCC, FPS, (width, height))\n",
        "\n",
        "# Train and test the model for a number of rounds\n",
        "avg_ep_lens = []\n",
        "avg_ep_rews = []\n",
        "for rnd in range(NUM_ROUNDS):\n",
        "\n",
        "    # Train the model\n",
        "    model.learn(total_timesteps=NUM_TRAINING_STEPS_PER_ROUND)\n",
        "\n",
        "    # Save the model\n",
        "    model.save(f\"{MODEL_FILENAME_BASE}_{rnd}\")\n",
        "\n",
        "    # Test the model in several episodes\n",
        "    avg_ep_len = 0\n",
        "    avg_ep_rew = 0\n",
        "    for ep in range(NUM_TESTS_PER_ROUND):\n",
        "\n",
        "        # Only record the first test\n",
        "        if ep == 0:\n",
        "            ep_len, ep_rew = test_model(env, model, video, f\"Round {rnd}\")\n",
        "        else:\n",
        "            ep_len, ep_rew = test_model(env, model)\n",
        "\n",
        "        # Accumulate average length and reward\n",
        "        avg_ep_len += ep_len\n",
        "        avg_ep_rew += ep_rew\n",
        "\n",
        "    # Record and dieplay average episode length and reward\n",
        "    avg_ep_len /= NUM_TESTS_PER_ROUND\n",
        "    avg_ep_lens.append(avg_ep_len)\n",
        "    avg_ep_rew /= NUM_TESTS_PER_ROUND\n",
        "    avg_ep_rews.append(avg_ep_rew)\n",
        "    print(f\"Round {rnd} | average test length: {avg_ep_len}, average test reward: {avg_ep_rew}\")\n",
        "\n",
        "# Close the video writer\n",
        "video.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93a8f342-a885-4b03-a03b-486477577489",
      "metadata": {
        "id": "93a8f342-a885-4b03-a03b-486477577489"
      },
      "outputs": [],
      "source": [
        "# Plot average test episode lengths and rewards for each round\n",
        "fig, axs = plt.subplots(1, 2)\n",
        "axs[0].plot(avg_ep_lens)\n",
        "axs[1].plot(avg_ep_rews)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c9f56a9-4830-4327-8aae-3881e9676162",
      "metadata": {
        "id": "4c9f56a9-4830-4327-8aae-3881e9676162"
      },
      "outputs": [],
      "source": [
        "# Model and video settings\n",
        "MODEL_FILENAME = \"pendulum-ppo_15\"\n",
        "VIDEO_FILENAME = \"3-testing.mp4\"\n",
        "\n",
        "# Load the model\n",
        "model = sb3.PPO.load(MODEL_FILENAME)\n",
        "\n",
        "# Create recorder\n",
        "video = cv2.VideoWriter(VIDEO_FILENAME, FOURCC, FPS, (width, height))\n",
        "\n",
        "# Test the model\n",
        "ep_len, ep_rew = test_model(env, model, video, MODEL_FILENAME)\n",
        "print(f\"Episode length: {ep_len}, reward: {ep_rew}\")\n",
        "\n",
        "# Close the video writer\n",
        "video.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f80a9e8b-501d-4306-8d6d-f8c1b988f4ec",
      "metadata": {
        "id": "f80a9e8b-501d-4306-8d6d-f8c1b988f4ec"
      },
      "outputs": [],
      "source": [
        "# We're done with the environment\n",
        "env.close()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}