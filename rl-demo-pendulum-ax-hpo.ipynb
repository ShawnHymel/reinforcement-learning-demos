{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3990b1ea-42c5-4fe4-9fd7-e709b9254ca1",
   "metadata": {},
   "source": [
    "# Pendulum RL Demo: Hyperparameter Optimization with Meta Ax\n",
    "\n",
    "[![Open In Colab <](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ShawnHymel/reinforcement-learning-demos/blob/main/rl-demo-pendulum-ax-hpo.ipynb)\n",
    "\n",
    "This example demonstrates Bayesian Optimization to find good hyperparameters using [Meta Ax](https://ax.dev/). Trial results are saved to [Weights & Biases](https://wandb.ai/site) so you can remotely watch training in real-time and later review the hyperparameters for each trial. We use the basic [gymnasium pendulum environment](https://gymnasium.farama.org/environments/classic_control/pendulum/) and [PPO from stable-baselines3](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html) as our reinforcement learning agent.\n",
    "\n",
    "Start with [this pendulum demo](https://github.com/ShawnHymel/reinforcement-learning-demos/blob/main/rl-demo-pendulum.ipynb) to see how RL training works. In that example, we pull hyperparameters from a known-good source ([rl-baselines3-zoo](https://github.com/DLR-RM/rl-baselines3-zoo)). But what happens when you don't have good hyperparameters to start with? RL can be tricky and vary wildly depending on what hyperparameters you use. So, hyperparameter tuning is very important. We use a combination of Ax (for Bayesian Optimization) and W&B (for experiment logging) to show how autotuning can be done (especially when model training is computationally expensive and time-consuming)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b03c626-ed19-4a89-8206-bf06e4e56f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install gymnasium==0.28.1\n",
    "# !python -m pip install stable-baselines3[extra]==2.1.0\n",
    "# !python -m pip install ax-platform==0.3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66e8d1a-e4c2-45ef-a123-698896b1e9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check versions\n",
    "import importlib.metadata\n",
    "\n",
    "print(f\"torch version: {importlib.metadata.version('torch')}\")\n",
    "print(f\"gymnasium version: {importlib.metadata.version('gymnasium')}\")\n",
    "print(f\"sb3 version: {importlib.metadata.version('stable-baselines3')}\")\n",
    "print(f\"cv2 version: {importlib.metadata.version('opencv-python')}\")\n",
    "print(f\"ax version: {importlib.metadata.version('ax-platform')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fb1f95-ee81-4d47-9761-ed9e1bacc30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import logging\n",
    "from typing import Any, Dict, Tuple, Union\n",
    "\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import cv2\n",
    "import wandb\n",
    "\n",
    "import stable_baselines3 as sb3\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.logger import KVWriter, Logger\n",
    "\n",
    "from ax.service.ax_client import AxClient\n",
    "from ax.service.managed_loop import optimize\n",
    "from ax.utils.notebook.plotting import render\n",
    "from ax.utils.tutorials.cnn_utils import train, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3661e6-9a0c-430a-802d-4bc95c81932a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log in to Weights & Biases\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075a089e-2835-46ca-990d-127add0d0cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make wandb be quiet\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "logger = logging.getLogger(\"wandb\")\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08abcee-0c9c-4595-9eca-240931aff795",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b535fb29-726a-4189-a2f7-2f888b9f4dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seeds(seed: int, using_cuda: bool = False) -> None:\n",
    "    \"\"\"\n",
    "    Seed the different random generators.\n",
    "    https://stable-baselines3.readthedocs.io/en/master/_modules/stable_baselines3/common/utils.html\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set seed for Python random, NumPy, and Torch\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    th.manual_seed(seed)\n",
    "\n",
    "    # Set deterministic operations for CUDA\n",
    "    if using_cuda:\n",
    "        th.backends.cudnn.deterministic = True\n",
    "        th.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8794a1eb-7ebf-4cfc-8e42-bed2957e70cb",
   "metadata": {},
   "source": [
    "## Set up environment\n",
    "\n",
    "Create pendulum environment from gymnasium. Test the environment using a dummy agent that simply predicts random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ccafdc-aab0-40d0-a7f4-17c0bf96befa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "# https://gymnasium.farama.org/api/env/\n",
    "env = gym.make('Pendulum-v1', render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888beb0f-6ed3-420b-ad91-3caefd70bf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the pendulum environment\n",
    "# https://gymnasium.farama.org/environments/classic_control/pendulum/\n",
    "obs, info = env.reset()\n",
    "print(obs)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac722bc-1386-42f5-b919-a0371e1f1578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render the environment (render is not the observation!) and get width and height\n",
    "frame = env.render()\n",
    "width = frame.shape[1]\n",
    "height = frame.shape[0]\n",
    "\n",
    "# Show frame\n",
    "print(frame.shape)\n",
    "plt.imshow(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283d7147-c506-4779-8dd4-0d821d6495d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that tests the model in the given environment\n",
    "def test_agent(env, model, max_steps=0, video=None, msg=None):\n",
    "\n",
    "    # Reset environment\n",
    "    obs, info = env.reset()\n",
    "    ep_len = 0\n",
    "    ep_rew = 0\n",
    "    avg_step_time = 0.0\n",
    "\n",
    "    # Run episode until complete\n",
    "    while True:\n",
    "\n",
    "        # Provide observation to policy to predict the next action\n",
    "        timestamp = time.time()\n",
    "        action, _ = model.predict(obs)\n",
    "\n",
    "        # Perform action, update total reward\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        avg_step_time += time.time() - timestamp\n",
    "        ep_rew += reward\n",
    "        \n",
    "        # Record frame to video\n",
    "        if video:\n",
    "            frame = env.render()\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "            frame = cv2.putText(\n",
    "                frame,                    # Image\n",
    "                msg,                      # Text to add\n",
    "                (10, 25),                 # Origin of text in imagg\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, # Font\n",
    "                1,                        # Font scale\n",
    "                (0, 0, 0,),               # Color\n",
    "                2,                        # Thickness\n",
    "                cv2.LINE_AA               # Line type\n",
    "            )\n",
    "            video.write(frame)\n",
    "\n",
    "        # Increase step counter\n",
    "        ep_len += 1\n",
    "        if (max_steps > 0) and (ep_len >= max_steps):\n",
    "            break\n",
    "\n",
    "        # Check to see if episode has ended\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "        \n",
    "    # Calculate average step time\n",
    "    avg_step_time /= ep_len\n",
    "    \n",
    "    return ep_len, ep_rew, avg_step_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646486ea-8ab6-4a18-86d3-ced7651e5d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyAgent():\n",
    "    \"\"\"\n",
    "    Agent that just predicts random actions\n",
    "    \"\"\"\n",
    "\n",
    "    # Save environment\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "\n",
    "    # Always output random action regardless of observation\n",
    "    def predict(self, obs):\n",
    "        action = self.env.action_space.sample()\n",
    "        return action, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970770b4-9f48-41a9-8ec4-6ed6bb93cfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recorder settings\n",
    "FPS = 30\n",
    "FOURCC = cv2.VideoWriter.fourcc('m', 'p', '4', 'v')\n",
    "VIDEO_FILENAME = \"1-random.mp4\"\n",
    "\n",
    "# Create recorder\n",
    "video = cv2.VideoWriter(VIDEO_FILENAME, FOURCC, FPS, (width, height))\n",
    "\n",
    "# Try running a few episodes with the environment and random actions\n",
    "dummy_agent = DummyAgent(env)\n",
    "for ep in range(5):\n",
    "    ep_len, ep_rew, step_time = test_agent(env, dummy_agent, max_steps=100, video=video, msg=f\"Random, episode {ep}\")\n",
    "    print(f\"Episode {ep} | Length: {ep_len}, Reward: {ep_rew}, Step time: {(step_time * 1000):.2e} ms\")\n",
    "    \n",
    "# Close the video writer\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86379700-764c-4a96-b3a3-d7554c95c8e5",
   "metadata": {},
   "source": [
    "## Testing and logging callbacks\n",
    "\n",
    "Construct custom callbacks for Stable-Baselines3 to test our agent and log metrics to Weights & Biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc684397-df3e-4061-b0aa-c3e3811346c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate agent on a number of tests\n",
    "def evaluate_agent(env, model, steps_per_test, num_tests):\n",
    "    \n",
    "    # Initialize metrics\n",
    "    avg_ep_len = 0\n",
    "    avg_ep_rew = 0\n",
    "    avg_step_time = 0.0\n",
    "    \n",
    "    # Test the agent a number of times\n",
    "    for ep in range(num_tests):\n",
    "        ep_len, ep_rew, step_time = test_agent(env, model, max_steps=steps_per_test)\n",
    "        avg_ep_len += ep_len\n",
    "        avg_ep_rew += ep_rew\n",
    "        avg_step_time += step_time\n",
    "        \n",
    "    # Compute metrics\n",
    "    avg_ep_len /= num_tests\n",
    "    avg_ep_rew /= num_tests\n",
    "    avg_step_time /= num_tests\n",
    "    \n",
    "    return avg_ep_len, avg_ep_rew, avg_step_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7c3354-2049-465d-ba7f-8a17510cfc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalAndSaveCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Evaluate and save the model every ``check_freq`` steps\n",
    "    \n",
    "    More info: https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html\n",
    "    \"\"\"\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(\n",
    "        self, \n",
    "        check_freq, \n",
    "        save_dir,\n",
    "        model_name=\"model\",\n",
    "        replay_buffer_name=None,\n",
    "        steps_per_test=0, \n",
    "        num_tests=10,\n",
    "        step_offset=0,\n",
    "        verbose=1,\n",
    "    ):\n",
    "        super(EvalAndSaveCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_dir = save_dir\n",
    "        self.model_name = model_name\n",
    "        self.replay_buffer_name = replay_buffer_name\n",
    "        self.num_tests = num_tests\n",
    "        self.steps_per_test = steps_per_test\n",
    "        self.step_offset = step_offset\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    # Create directory for saving the models\n",
    "    def _init_callback(self):\n",
    "        if self.save_dir is not None:\n",
    "            os.makedirs(self.save_dir, exist_ok=True)\n",
    "            \n",
    "    # Save and evaluate model at a set interval\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            \n",
    "            # Set actual number of steps (including offset)\n",
    "            actual_steps = self.step_offset + self.n_calls\n",
    "            \n",
    "            # Save model\n",
    "            model_path = os.path.join(self.save_dir, f\"{self.model_name}_{str(actual_steps)}\")\n",
    "            self.model.save(model_path)\n",
    "            \n",
    "            # Save replay buffer\n",
    "            if self.replay_buffer_name != None:\n",
    "                replay_buffer_path = os.path.join(self.save_dir, f\"{self.replay_buffer_name}\")\n",
    "                self.model.save_replay_buffer(replay_buffer_path)\n",
    "            \n",
    "            # Evaluate the agent\n",
    "            avg_ep_len, avg_ep_rew, avg_step_time = evaluate_agent(\n",
    "                env, \n",
    "                self.model, \n",
    "                self.steps_per_test, \n",
    "                self.num_tests\n",
    "            )\n",
    "            if self.verbose:\n",
    "                print(f\"{str(actual_steps)} steps | average test length: {avg_ep_len}, average test reward: {avg_ep_rew}\")\n",
    "                \n",
    "            # Log metrics to WandB\n",
    "            log_dict = {\n",
    "                'avg_ep_len': avg_ep_len,\n",
    "                'avg_ep_rew': avg_ep_rew,\n",
    "                'avg_step_time': avg_step_time,\n",
    "            }\n",
    "            wandb.log(log_dict, commit=True, step=actual_steps)\n",
    "            \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5293cee-c6d3-4b64-939c-6e53c00ea94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WandBWriter(KVWriter):\n",
    "    \"\"\"\n",
    "    Log metrics to Weights & Biases when called by .learn()\n",
    "    \n",
    "    More info: https://stable-baselines3.readthedocs.io/en/master/_modules/stable_baselines3/common/logger.html#KVWriter\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize run\n",
    "    def __init__(self, run, verbose=1):\n",
    "        super().__init__()\n",
    "        self.run = run\n",
    "        self.verbose = verbose\n",
    "\n",
    "    # Write metrics to W&B project\n",
    "    def write(self, \n",
    "              key_values: Dict[str, Any], \n",
    "              key_excluded: Dict[str, Union[str, Tuple[str, ...]]], \n",
    "              step: int = 0) -> None:\n",
    "        log_dict = {}\n",
    "        \n",
    "        # Go through each key/value pairs\n",
    "        for (key, value), (_, excluded) in zip(\n",
    "            sorted(key_values.items()), sorted(key_excluded.items())):\n",
    "            \n",
    "            if self.verbose >= 2:\n",
    "                print(f\"step={step} | {key} : {value} ({type(value)})\")\n",
    "            \n",
    "            # Skip excluded items\n",
    "            if excluded is not None and \"wandb\" in excluded:\n",
    "                continue\n",
    "                \n",
    "            # Log integers and floats\n",
    "            if isinstance(value, np.ScalarType):\n",
    "                if not isinstance(value, str):\n",
    "                    wandb.log(data={key: value}, step=step)\n",
    "                    log_dict[key] = value\n",
    "                \n",
    "        # Print to console\n",
    "        if self.verbose >= 1:\n",
    "            print(f\"Log for steps={step}\")\n",
    "            print(f\"--------------\")\n",
    "            for (key, value) in sorted(log_dict.items()):\n",
    "                print(f\"  {key}: {value}\")\n",
    "            print()\n",
    "                \n",
    "    # Close the W&B run\n",
    "    def close(self) -> None:\n",
    "        self.run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f898db-18cc-4ea2-84e0-7435a6ba90bc",
   "metadata": {},
   "source": [
    "## Define train and test function for a single trial\n",
    "\n",
    "A single \"trial\" is fully training and then testing the agent using one set of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b8c34f-bc3b-40a9-8a50-ae9f469e3dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_trial(settings, hparams):\n",
    "    \"\"\"\n",
    "    Training loop used to evaluate a set of hyperparameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set random seed\n",
    "    set_random_seeds(settings['seed'], using_cuda=th.cuda.is_available())\n",
    "    \n",
    "    # Create new W&B run\n",
    "    config = {}\n",
    "    dt = datetime.datetime.now(datetime.timezone.utc)\n",
    "    dt = dt.replace(microsecond=0, tzinfo=None)\n",
    "    run = wandb.init(\n",
    "        project=settings['wandb_project'], \n",
    "        name=str(dt), \n",
    "        config=config,\n",
    "        settings=wandb.Settings(silent=(not settings['verbose_wandb']))\n",
    "    )\n",
    "\n",
    "    # Print run info\n",
    "    if settings['verbose_trial'] > 0:\n",
    "        print(f\"WandB run ID: {run.id}\")\n",
    "        print(f\"WandB run name: {run.name}\")\n",
    "    \n",
    "    # Log hyperparameters to W&B\n",
    "    wandb.config.update(hparams)\n",
    "    \n",
    "    # Set custom logger with our custom writer\n",
    "    wandb_writer = WandBWriter(run, verbose=settings['verbose_log'])\n",
    "    loggers = Logger(\n",
    "        folder=None,\n",
    "        output_formats=[wandb_writer]\n",
    "    )\n",
    "    \n",
    "    # Calculate derived hyperparameters\n",
    "    n_steps = 2 ** hparams['steps_per_update_pow2']\n",
    "    minibatch_size = 2 ** hparams['minibatch_size_pow2']\n",
    "    layer_1 = 2 ** hparams['layer_1_pow2']\n",
    "    layer_2 = 2 ** hparams['layer_2_pow2']\n",
    "    \n",
    "    \n",
    "    # Set completed steps to checkpoint number (in filename) or 0 to start over\n",
    "    # TODO: how to resume if trial is paused?\n",
    "    completed_steps = 0\n",
    "\n",
    "    # Load model or create new model\n",
    "    # PPO docs: https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html\n",
    "    # Policy networks: https://stable-baselines.readthedocs.io/en/master/modules/policies.html\n",
    "    if completed_steps != 0:\n",
    "        model_path = os.path.join(settings['save_dir'], f\"{settings['model_name']}_{str(completed_steps)}.zip\")\n",
    "        model = sb3.PPO.load(model_path, env)\n",
    "        steps_to_complete = settings['total_steps'] - completed_steps\n",
    "    else:\n",
    "        model = sb3.PPO(\n",
    "            'MlpPolicy',\n",
    "            env,\n",
    "            learning_rate=hparams['learning_rate'], # Learning rate of neural network (default: 0.0003)\n",
    "            n_steps=n_steps,                        # Number of steps per update (default: 2048)\n",
    "            batch_size=minibatch_size,              # Minibatch size for NN update (default: 64)\n",
    "            gamma=hparams['gamma'],                 # Discount factor (default: 0.99)\n",
    "            ent_coef=hparams['entropy_coef'],       # Entropy, how much to explore (default: 0.0)\n",
    "            use_sde=hparams['use_sde'],             # Use generalized State Dependent Exploration (default: False)\n",
    "            sde_sample_freq=hparams['sde_freq'],    # Number of steps before sampling new noise matrix (default -1)\n",
    "            policy_kwargs={'net_arch': [layer_1, layer_2]}, # (default: [64, 64])\n",
    "            verbose=settings['verbose_train']       # Print training metrics (default: 0)\n",
    "        )\n",
    "        steps_to_complete = settings['total_steps']\n",
    "        \n",
    "    # Set up checkpoint callback\n",
    "    checkpoint_callback = EvalAndSaveCallback(\n",
    "        check_freq=settings['checkpoint_freq'], \n",
    "        save_dir=settings['save_dir'],\n",
    "        model_name=settings['model_name'],\n",
    "        replay_buffer_name=settings['replay_buffer_name'],\n",
    "        steps_per_test=settings['steps_per_test'],\n",
    "        num_tests=settings['tests_per_check'],\n",
    "        step_offset=(settings['total_steps'] - steps_to_complete),\n",
    "        verbose=settings['verbose_test'],\n",
    "    )\n",
    "    \n",
    "    # Choo choo train\n",
    "    model.learn(total_timesteps=steps_to_complete, \n",
    "                callback=[checkpoint_callback])\n",
    "    \n",
    "    # Get dataframe of run metrics\n",
    "    history = wandb.Api().run(f\"{run.project}/{run.id}\").history()\n",
    "\n",
    "    # Get index of evaluation with maximum reward\n",
    "    max_idx = np.argmax(history.loc[:, 'avg_ep_rew'].values)\n",
    "\n",
    "    # Find number of steps required to produce that maximum reward\n",
    "    max_rew_steps = history['_step'][max_idx]\n",
    "    if settings['verbose_trial'] > 0:\n",
    "        print(f\"Steps with max reward: {max_rew_steps}\")\n",
    "    \n",
    "    # Load model with maximum reward from previous run\n",
    "    model_path = os.path.join(settings['save_dir'], f\"{settings['model_name']}_{str(max_rew_steps)}.zip\")\n",
    "    model = sb3.PPO.load(model_path, env)\n",
    "    \n",
    "    # Evaluate the agent\n",
    "    avg_ep_len, avg_ep_rew, avg_step_time = evaluate_agent(\n",
    "        env, \n",
    "        model, \n",
    "        settings['steps_per_test'],\n",
    "        settings['tests_per_check'],\n",
    "    )\n",
    "    \n",
    "    # Log final evaluation metrics to WandB run\n",
    "    wandb.run.summary['Average test episode length'] = avg_ep_len\n",
    "    wandb.run.summary['Average test episode reward'] = avg_ep_rew\n",
    "    wandb.run.summary['Average test step time'] = avg_step_time\n",
    "    \n",
    "    # Print final run metrics\n",
    "    if settings['verbose_trial'] > 0:\n",
    "        print(\"---\")\n",
    "        print(f\"Best model: {settings['model_name']}_{str(max_rew_steps)}.zip\")\n",
    "        print(f\"Average episode length: {avg_ep_len}\")\n",
    "        print(f\"Average episode reward: {avg_ep_rew}\")\n",
    "        print(f\"Average step time: {avg_step_time}\")\n",
    "                      \n",
    "    # Close W&B run\n",
    "    run.finish()\n",
    "    \n",
    "    return avg_ep_rew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce30c27a-7d64-4af2-a2b8-9d142c418554",
   "metadata": {},
   "source": [
    "## Perform trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87c458c-6d9e-4b4d-af9a-e8f4a8899b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project settings that do not change\n",
    "settings = {\n",
    "    'wandb_project': \"pendulum-ax-test\",\n",
    "    'model_name': \"ppo-pendulum\",\n",
    "    'ax_experiment_name': \"ppo-pendulum-experiment\",\n",
    "    'ax_objective_name': \"avg_ep_rew\",\n",
    "    'replay_buffer_name': None,\n",
    "    'save_dir': \"checkpoints\",\n",
    "    'checkpoint_freq': 10_000,\n",
    "    'steps_per_test': 100,\n",
    "    'tests_per_check': 10,\n",
    "    'total_steps': 100_000,\n",
    "    'num_trials': 60,\n",
    "    'seed': 42,\n",
    "    'verbose_ax': False,\n",
    "    'verbose_wandb': False,\n",
    "    'verbose_train': 0,\n",
    "    'verbose_log': 0,\n",
    "    'verbose_test': 0,\n",
    "    'verbose_trial': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9381d59b-cfd5-4e9c-bcea-d518ff634da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters we want to optimize\n",
    "# Ref: https://github.com/facebook/Ax/blob/6443cee30cbf8cec290200a7420a3db08e4b5445/ax/service/ax_client.py#L236\n",
    "# Example: https://github.com/facebook/Ax/blob/main/tutorials/tune_cnn_service.ipynb\n",
    "# Hyperparameters: https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#stable_baselines3.ppo.PPO\n",
    "hparams = [\n",
    "    {\n",
    "        'name': \"n_envs\",\n",
    "        'type': \"fixed\",\n",
    "        'value_type': \"int\",\n",
    "        'value': 1,\n",
    "    },\n",
    "    {\n",
    "        'name': \"learning_rate\",\n",
    "        'type': \"range\",\n",
    "        'value_type': \"float\",\n",
    "        'bounds': [1e-5, 1e-2],\n",
    "        'log_scale': True,\n",
    "    },\n",
    "    {\n",
    "        'name': \"steps_per_update_pow2\",\n",
    "        'type': \"range\",\n",
    "        'value_type': \"int\",\n",
    "        'bounds': [6, 12], # Inclusive, 2**n between [64, 4096]\n",
    "        'log_scale': False,\n",
    "        'is_ordered': False,\n",
    "    },\n",
    "    {\n",
    "        'name': \"minibatch_size_pow2\",\n",
    "        'type': \"range\",\n",
    "        'value_type': \"int\",\n",
    "        'bounds': [5, 10], # Inclusive, 2**n between [32, 1024]\n",
    "        'log_scale': False,\n",
    "        'is_ordered': False,\n",
    "    },\n",
    "    {\n",
    "        'name': \"gamma\",\n",
    "        'type': \"range\",\n",
    "        'value_type': \"float\",\n",
    "        'bounds': [0.9, 0.99],\n",
    "        'log_scale': False,\n",
    "    },\n",
    "    {\n",
    "        'name': \"entropy_coef\",\n",
    "        'value_type': \"float\",\n",
    "        'type': \"range\",\n",
    "        'bounds': [0.0, 0.1],\n",
    "        'log_scale': False,\n",
    "    },\n",
    "    {\n",
    "        'name': \"use_sde\",\n",
    "        'value_type': \"bool\",\n",
    "        'type': \"choice\",\n",
    "        'values': [True, False],\n",
    "        'is_ordered': False,\n",
    "    },\n",
    "    {\n",
    "        'name': \"sde_freq\",\n",
    "        'type': \"range\",\n",
    "        'value_type': \"int\",\n",
    "        'bounds': [-1, 8],\n",
    "        'log_scale': False,\n",
    "    },\n",
    "    {\n",
    "        'name': \"layer_1_pow2\",\n",
    "        'type': \"range\",\n",
    "        'value_type': \"int\",\n",
    "        'bounds': [5, 8], # Inclusive, 2**n between [32, 256]\n",
    "        'log_scale': False,\n",
    "        'is_ordered': False,\n",
    "    },\n",
    "    {\n",
    "        'name': \"layer_2_pow2\",\n",
    "        'type': \"range\",\n",
    "        'value_type': \"int\",\n",
    "        'bounds': [5, 8], # Inclusive, 2**n between [32, 256]\n",
    "        'log_scale': False,\n",
    "        'is_ordered': False,\n",
    "    },\n",
    "]\n",
    "\n",
    "# Set parameter constraints\n",
    "# Example: https://github.com/facebook/Ax/issues/621\n",
    "parameter_constraints = [\n",
    "    \"minibatch_size_pow2 >= steps_per_update_pow2\" # `batch_size` should be a factor of `n_steps * n_envs`, assume n_envs=1\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b899e21-fb75-40fd-a993-20daca29f37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our environment\n",
    "try:\n",
    "    env.close()\n",
    "except NameError:\n",
    "    pass\n",
    "env = gym.make('Pendulum-v1', render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0deb7947-5b65-4ea2-a76f-f43eeb44438c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosntruct path to Ax experiment snapshot file\n",
    "ax_snapshot_path = os.path.join(settings['save_dir'], f\"{settings['ax_experiment_name']}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fa574f-6ca8-4f17-973e-ec5d357b73de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DANGER! Uncomment to delete the experiment file to start over\n",
    "# os.remove(ax_snapshot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7661e759-287b-412b-a371-c5dd0f7f8323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experiment from snapshot if it exists, otherwise create a new one\n",
    "# Ref: https://ax.dev/versions/0.2.10/api/service.html#ax.service.ax_client.AxClient.create_experiment\n",
    "if os.path.exists(ax_snapshot_path):\n",
    "    print(f\"Loading experiment from snapshot: {ax_snapshot_path}\")\n",
    "    ax_client = AxClient.load_from_json_file(ax_snapshot_path)\n",
    "else:\n",
    "    print(f\"Creating new experiment. Snapshot to be saved at {ax_snapshot_path}.\")\n",
    "    ax_client = AxClient(\n",
    "        random_seed=settings['seed'],\n",
    "        verbose_logging=settings['verbose_ax'],\n",
    "    )\n",
    "    ax_client.create_experiment(\n",
    "        name=settings['ax_experiment_name'],\n",
    "        parameters=hparams,\n",
    "        objective_name=settings['ax_objective_name'],\n",
    "        minimize=False,\n",
    "        parameter_constraints=parameter_constraints,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122edfe0-6999-46fb-97ad-c91c280250eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choo choo! Perform trials to optimize hyperparameters\n",
    "while True:\n",
    "    \n",
    "    # Get next hyperparameters and end experiment if we've reached max trials\n",
    "    next_hparams, trial_index = ax_client.get_next_trial()\n",
    "    if trial_index >= settings['num_trials']:\n",
    "        break\n",
    "        \n",
    "    # Show that we're starting a new trial\n",
    "    if settings['verbose_trial'] > 0:\n",
    "        print(f\"--- Trial {trial_index} ---\")\n",
    "        \n",
    "    # Perform trial\n",
    "    avg_ep_rew = do_trial(settings, next_hparams)\n",
    "    ax_client.complete_trial(\n",
    "        trial_index=trial_index,\n",
    "        raw_data=avg_ep_rew,\n",
    "    )\n",
    "    \n",
    "    # Save experiment snapshot\n",
    "    ax_client.save_to_json_file(ax_snapshot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66aad847-19f7-4109-8f4a-ca98b07c50b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd5b7fd-83d9-4842-9b6d-039048f777b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dummy run so we can programmatically obtain the W&B username\n",
    "wandb_api = wandb.Api()\n",
    "dummy_run = wandb.init()\n",
    "wandb_entity = wandb.run.entity\n",
    "dummy_id = dummy_run.id\n",
    "dummy_run.finish()\n",
    "# wandb_run = wandb_api.run(f\"{wandb_entity}/{settings['wandb_project']}/{dummy_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f149866e-653d-4547-8f09-4604399850f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of average test reward for each trial\n",
    "runs = wandb_api.runs(path=f\"{wandb_entity}/{settings['wandb_project']}\")\n",
    "trial_rews = []\n",
    "for r in reversed(runs):\n",
    "    trial_rew = r.summary.get('Average test episode reward')\n",
    "    if trial_rew is not None:\n",
    "        trial_rews.append(trial_rew)\n",
    "        \n",
    "# Plot the list of test rewards over time\n",
    "plt.plot(trial_rews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95f4e7b-81d2-446b-b35a-9b80f9f5749b",
   "metadata": {},
   "source": [
    "## Train agent with best hyperparameters\n",
    "\n",
    "Note that you may need to manually adjust some of the hyperparameters to make everything work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbe2576-b354-4a29-af68-7ce5bc78b3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get from W&B dashboard\n",
    "hparams = {\n",
    "    'n_envs': 1,\n",
    "    'learning_rate': 0.0078,\n",
    "    'steps_per_update_pow2': 10,\n",
    "    'minibatch_size_pow2': 10,\n",
    "    'gamma': 0.974,\n",
    "    'entropy_coef': 0.0356,\n",
    "    'use_sde': True,\n",
    "    'sde_freq': 4,\n",
    "    'layer_1_pow2': 5,\n",
    "    'layer_2_pow2': 8,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fb486e-b0fe-459e-abe2-ccde3bb77133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our environment\n",
    "try:\n",
    "    env.close()\n",
    "except NameError:\n",
    "    pass\n",
    "env = gym.make('Pendulum-v1', render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ba77d0-89ea-409e-82f7-738cc491c438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "_ = do_trial(settings, hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc5eb17-f79a-46ea-90b7-080f758c4b92",
   "metadata": {},
   "source": [
    "## Test fully trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5045e5ca-843b-4027-b312-53981020c929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and video settings\n",
    "MODEL_FILENAME = \"checkpoints/ppo-pendulum_90000.zip\"\n",
    "VIDEO_FILENAME = \"2-testing.mp4\"\n",
    "\n",
    "# Create our environment\n",
    "try:\n",
    "    env.close()\n",
    "except NameError:\n",
    "    pass\n",
    "env = gym.make('Pendulum-v1', render_mode='rgb_array')\n",
    "\n",
    "# Load the model\n",
    "model = sb3.PPO.load(MODEL_FILENAME)\n",
    "\n",
    "# Create recorder\n",
    "video = cv2.VideoWriter(VIDEO_FILENAME, FOURCC, FPS, (width, height))\n",
    "\n",
    "# Test the model\n",
    "ep_len, ep_rew, avg_step_time = test_agent(env, model, max_steps=200, video=video, msg=f\"{MODEL_FILENAME}\")\n",
    "print(f\"Episode length: {ep_len}, reward: {ep_rew}, avg step time: {avg_step_time}\")\n",
    "\n",
    "# Close the video writer\n",
    "video.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
